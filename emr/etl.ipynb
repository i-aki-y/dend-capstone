{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, flatten, explode, collect_list, concat_ws\n",
    "from pyspark.sql.functions import (year, month, dayofmonth, hour, weekofyear, date_format,\n",
    "                                   to_timestamp, from_unixtime)\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create spark session\"\"\"\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f11248dc2e8>"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â set project S3 bucket\n",
    "bucket = \"akyk-dend-capstone\"\n",
    "\n",
    "# input data path in s3 \n",
    "tmdb_dir = \"tmdb-data\"\n",
    "ml_dir = \"ml-latest-201908\"\n",
    "\n",
    "#  output data path in s3\n",
    "output_dir = \"result\"\n",
    "\n",
    "update_prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_s3(client, bucket, prefix):\n",
    "    \"\"\"Check whether the given s3 path exist or not\n",
    "    client: s3 client\n",
    "    bucket: s3 bucket to check\n",
    "    prefix: Returns `True` when found item which matches to the `prefix`    \n",
    "    \"\"\"    \n",
    "    res = client.list_objects(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix\n",
    "    )\n",
    "    \n",
    "    return \"Contents\" in res\n",
    "\n",
    "def merge_dataset(df_latest, df_update, client, bucket, tmp_path, latest_path):\n",
    "    \"\"\"Merge latest data and additional data\n",
    "    df_latest: Dataframe of latest dataset\n",
    "    df_update: Dataframe of addition dataset\n",
    "    client: s3 client\n",
    "    bucket: Project s3 bucket\n",
    "    tmp_path: Temporary path name used to save merged data\n",
    "    latest_path: Path of latest data is saved\n",
    "    \"\"\"\n",
    "    \n",
    "    df_latest.createOrReplaceTempView(\"data_latest\")\n",
    "    df_update.createOrReplaceTempView(\"data_update\")\n",
    "    \n",
    "    # Data merge is done by a following steps\n",
    "    # 1. Filter latest data with id which included in update data\n",
    "    # 2. Concate update data\n",
    "    df_merged = spark.sql(\"\"\"\n",
    "select\n",
    "    *\n",
    "from\n",
    "    data_latest\n",
    "where id not in (select id from data_update)\n",
    "\n",
    "union\n",
    "\n",
    "select\n",
    "    *\n",
    "from\n",
    "    data_update\n",
    "\n",
    "\"\"\")\n",
    "    \n",
    "    df_merged.persist()\n",
    "    print(\"merged dataset\")\n",
    "    print(\"size: \",df_merged.count())    \n",
    "    print(\"Write merged data to: \", latest_path)\n",
    "    df_merged.write.mode('overwrite').parquet(f\"s3a://{bucket}/{latest_path}\")\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge latest TMDB data with additional data \n",
    "\n",
    "the following process merge the latest TMDB data with additional TMDB data which uploaded in update folder in project s3 bucket.\n",
    "The merged data is saved as the latest TMDB date and previous latest data is overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_latest(spark, bucket, tmdb_dir, update_prefix):\n",
    "\n",
    "    s3client = boto3.client('s3')\n",
    "    \n",
    "    latest_name = \"latest.parquet\"    \n",
    "    tmp_path = f\"{tmdb_dir}/tmp.parquet\"\n",
    "    latest_path = f\"{tmdb_dir}/{latest_name}\"\n",
    "    has_latest = check_s3(s3client, bucket,  latest_path)\n",
    "    has_update =  check_s3(s3client, bucket, f\"{tmdb_dir}/update/{update_prefix}\")\n",
    "\n",
    "    if has_latest:\n",
    "        print(\"Load latest data\")\n",
    "        df_tmdb_latest = spark.read.parquet(f\"s3a://{bucket}/{tmdb_dir}/{latest_name}\").persist()\n",
    "        print(\"size: \", df_tmdb_latest.count())\n",
    "    if has_update:\n",
    "        ## update data are uploaded as json files\n",
    "        print(\"Load update files\")\n",
    "        df_tmdb_update = spark.read.json(f\"s3a://{bucket}/{tmdb_dir}/update/{update_prefix}*\").persist()\n",
    "        print(\"size: \", df_tmdb_update.count())\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if has_latest and has_update:\n",
    "        print(\"merge data\")        \n",
    "        df_tmdb_latest = merge_dataset(df_tmdb_latest, df_tmdb_update, s3client, bucket, tmp_path, latest_path)\n",
    "\n",
    "    elif has_latest and not has_update:\n",
    "        print(\"no merge\")    \n",
    "        pass\n",
    "\n",
    "    elif (not has_latest) and has_update:\n",
    "        print(\"Any latest data is not found. Use update data as latest\")\n",
    "        df_tmdb_latest = df_tmdb_update\n",
    "\n",
    "        write_path = f\"s3a://{bucket}/{tmdb_dir}/{latest_name}\"\n",
    "\n",
    "        print(\"Write to: \", write_path)\n",
    "        df_tmdb_latest.write.mode('overwrite').parquet(write_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise OSError(\"no dataset found\")\n",
    "\n",
    "    return df_tmdb_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load latest data\n",
      "size:  57267\n",
      "Load update files\n",
      "size:  57267\n",
      "merge data\n",
      "merged dataset\n",
      "size:  57267\n",
      "Write merged data to:  tmdb-data/latest.parquet\n",
      "tmdb data count:  57267\n",
      "root\n",
      " |-- adult: boolean (nullable = true)\n",
      " |-- backdrop_path: string (nullable = true)\n",
      " |-- belongs_to_collection: struct (nullable = true)\n",
      " |    |-- backdrop_path: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- poster_path: string (nullable = true)\n",
      " |-- budget: long (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- keywords: struct (nullable = true)\n",
      " |    |-- keywords: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      " |-- production_companies: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- logo_path: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- origin_country: string (nullable = true)\n",
      " |-- production_countries: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- iso_3166_1: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: long (nullable = true)\n",
      " |-- reviews: struct (nullable = true)\n",
      " |    |-- page: long (nullable = true)\n",
      " |    |-- results: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- author: string (nullable = true)\n",
      " |    |    |    |-- content: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- total_pages: long (nullable = true)\n",
      " |    |-- total_results: long (nullable = true)\n",
      " |-- runtime: long (nullable = true)\n",
      " |-- spoken_languages: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- iso_639_1: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- video: boolean (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "df_tmdb = merge_latest(spark,  bucket, tmdb_dir, update_prefix)\n",
    "\n",
    "print(\"tmdb data count: \", df_tmdb.count())\n",
    "\n",
    "df_tmdb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read links.csv\n",
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- imdbId: string (nullable = true)\n",
      " |-- tmdbId: string (nullable = true)\n",
      "\n",
      "read movies.csv\n",
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n",
      "read ratings.csv\n",
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "create temp views"
     ]
    }
   ],
   "source": [
    "print(\"read links.csv\")\n",
    "df_link = spark.read.csv(f\"s3a://{bucket}/{ml_dir}/links.csv\", header=True)\n",
    "df_link.printSchema()\n",
    "\n",
    "print(\"read movies.csv\")\n",
    "df_ml_movie = spark.read.csv(f\"s3a://{bucket}/{ml_dir}/movies.csv\", header=True)\n",
    "df_ml_movie.printSchema()\n",
    "\n",
    "\n",
    "print(\"read ratings.csv\")\n",
    "df_ml_rate = spark.read.csv(f\"s3a://{bucket}/{ml_dir}/ratings.csv\", header=True)\n",
    "df_ml_rate.printSchema()\n",
    "\n",
    "print(\"create temp views\")\n",
    "df_tmdb.withColumn(\"release_date\", to_timestamp(\"release_date\", \"yyyy-MM-dd\")).createOrReplaceTempView(\"tmdb_data\")\n",
    "df_link.createOrReplaceTempView(\"link_data\")\n",
    "df_ml_movie.createOrReplaceTempView(\"ml_movie_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ml_movie_id: string (nullable = true)\n",
      " |-- tmdb_movie_id: long (nullable = true)\n",
      " |-- ml_genre: string (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- release_month: integer (nullable = true)\n",
      " |-- release_day: integer (nullable = true)\n",
      " |-- budget: long (nullable = true)\n",
      " |-- revenue: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_movie = spark.sql(\"\"\"\n",
    "select \n",
    "    ln.movieId as ml_movie_id\n",
    "    , tmdb.id as tmdb_movie_id\n",
    "    , ml.genres as ml_genre\n",
    "    , year(release_date) as release_year\n",
    "    , month(release_date) as release_month\n",
    "    , dayofmonth(release_date) as release_day\n",
    "    , tmdb.budget as budget\n",
    "    , tmdb.revenue as revenue\n",
    "    , tmdb.title as title\n",
    "    , vote_average as vote_average\n",
    "    , vote_count as vote_count\n",
    "    from link_data as ln\n",
    "    inner join tmdb_data as tmdb on ln.tmdbId = tmdb.id\n",
    "    inner join ml_movie_data as ml on ml.movieId = ln.movieId\n",
    "\"\"\")\n",
    "\n",
    "df_movie.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie.write.mode('overwrite').parquet(f\"s3a://{BUCKET}/{OUTPUT_DIR}/movies.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- kw: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_tmdb.select(\"id\", explode(\"keywords.keywords\").alias(\"kw\") ).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- keywords: string (nullable = false)\n",
      "\n",
      "+----+--------------------+\n",
      "|  id|            keywords|\n",
      "+----+--------------------+\n",
      "|  26|berlin germany ga...|\n",
      "| 964|paris france oper...|\n",
      "|1677|black people loss...|\n",
      "|1697|loss of loved one...|\n",
      "|1950|poker sport las v...|\n",
      "+----+--------------------+"
     ]
    }
   ],
   "source": [
    "df_keywords = spark.sql(\"\"\"\n",
    "select\n",
    "    id\n",
    "    , concat_ws(' ', collect_list(keyword)) as keywords\n",
    "from (\n",
    "    select\n",
    "        id\n",
    "        , keyword.name as keyword\n",
    "    from (\n",
    "    select \n",
    "        id\n",
    "        , explode(keywords.keywords) as keyword\n",
    "        from tmdb_data\n",
    "        ) as tmp\n",
    "    ) as tmp2\n",
    "group by id\n",
    "\"\"\")\n",
    "\n",
    "df_keywords.printSchema()\n",
    "\n",
    "df_keywords.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords.createOrReplaceTempView(\"keyword_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tmdb_id: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- keywords: string (nullable = false)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- overview: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_search = spark.sql(\"\"\"\n",
    "select\n",
    "    tmdb_data.id as tmdb_id\n",
    "    , title\n",
    "    , keyword_data.keywords\n",
    "    , tagline\n",
    "    , overview\n",
    "from tmdb_data\n",
    "inner join keyword_data on keyword_data.id = tmdb_data.id\n",
    "\"\"\")\n",
    "\n",
    "df_search.printSchema()\n",
    "df_search.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search.write.mode('overwrite').parquet(f\"s3a://{BUCKET}/{OUTPUT_DIR}/movie_for_search.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    307|   3.5|1256677221|\n",
      "|     1|    481|   3.5|1256677456|\n",
      "|     1|   1091|   1.5|1256677471|\n",
      "|     1|   1257|   4.5|1256677460|\n",
      "|     1|   1449|   4.5|1256677264|\n",
      "+------+-------+------+----------+"
     ]
    }
   ],
   "source": [
    "df_ml_rate.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+-------------------+\n",
      "|userId|movieId|rating| timestamp|                 ts|\n",
      "+------+-------+------+----------+-------------------+\n",
      "|     1|    307|   3.5|1256677221|2009-10-27 21:00:21|\n",
      "|     1|    481|   3.5|1256677456|2009-10-27 21:04:16|\n",
      "|     1|   1091|   1.5|1256677471|2009-10-27 21:04:31|\n",
      "|     1|   1257|   4.5|1256677460|2009-10-27 21:04:20|\n",
      "|     1|   1449|   4.5|1256677264|2009-10-27 21:01:04|\n",
      "+------+-------+------+----------+-------------------+"
     ]
    }
   ],
   "source": [
    "df_ml_rate.withColumn(\"ts\", from_unixtime(\"timestamp\")).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate log_data view\n",
    "df_ml_rate.withColumn(\"ts\", from_unixtime(\"timestamp\")).createOrReplaceTempView(\"ml_rate_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+----------+----+-----+---+----+----+-------+\n",
      "| timestamp|year|month|day|hour|week|weekday|\n",
      "+----------+----+-----+---+----+----+-------+\n",
      "|1493407021|2017|    4| 28|  19|  17|      6|\n",
      "|1493406667|2017|    4| 28|  19|  17|      6|\n",
      "|1493405824|2017|    4| 28|  18|  17|      6|\n",
      "|1493401234|2017|    4| 28|  17|  17|      6|\n",
      "|1117764448|2005|    6|  3|   2|  22|      6|\n",
      "+----------+----+-----+---+----+----+-------+"
     ]
    }
   ],
   "source": [
    "# extract columns to create time table\n",
    "df_rating_time = spark.sql(\"\"\"\n",
    "SELECT DISTINCT \n",
    "    timestamp\n",
    "    , year(ts) as year\n",
    "    , month(ts) as month\n",
    "    , cast(date_format(ts, \"dd\") as INTEGER) as day\n",
    "    , cast(date_format(ts, \"HH\") as INTEGER) as hour\n",
    "    , weekofyear(ts) as week \n",
    "    , dayofweek(ts) as weekday\n",
    "FROM ml_rate_data\n",
    "\"\"\")\n",
    "\n",
    "df_rating_time.printSchema()\n",
    "df_rating_time.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|year|  count|\n",
      "+----+-------+\n",
      "|2003| 895529|\n",
      "|2007|1067537|\n",
      "|2018| 994970|\n",
      "|2015|1718546|\n",
      "|2006|1181392|\n",
      "|2013| 624558|\n",
      "|1997| 339290|\n",
      "|2014| 575015|\n",
      "|2004|1168738|\n",
      "|1996| 817334|\n",
      "|1998| 148473|\n",
      "|2012| 774572|\n",
      "|2009| 967277|\n",
      "|2016|1862618|\n",
      "|1995|      2|\n",
      "|2001| 651806|\n",
      "|2005|1710357|\n",
      "|2000| 936235|\n",
      "|2010| 958032|\n",
      "|2011| 820000|\n",
      "+----+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df_rating_time.groupby(\"year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating_time.write.mode('overwrite').partitionBy(\"year\").parquet(f\"s3a://{BUCKET}/{OUTPUT_DIR}/rating_time.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "df_result_movie = spark.read.parquet(f\"s3a://{bucket}/{output_dir}/movies.parquet\").persist()\n",
    "df_result_rating_time = spark.read.parquet(f\"s3a://{bucket}/{output_dir}/rating_time.parquet\").persist()\n",
    "df_result_search = spark.read.parquet(f\"s3a://{bucket}/{output_dir}/movie_for_search.parquet\").persist()\n",
    "\n",
    "\n",
    "df_ml_movie =  spark.read.csv(f\"s3a://{bucket}/{ml_dir}/movies.csv\").persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result file not empty\n",
    "assert df_result_movie.count() > 0, \"result movie data is empty\"\n",
    "assert df_result_rating_time.count() > 0, \"result rating time data is empty\"\n",
    "assert df_result_search.count() > 0, \"result search data is empty\"\n",
    "\n",
    "print(\"All result data are not empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "assert df_result_movie.count() > 0, \"result movie data is empty\"\n",
    "assert df_result_rating_time.count() > 0, \"result rating time data is empty\"\n",
    "assert df_result_search.count() > 0, \"result search data is empty\"\n",
    "\n",
    "print(\"All result data are not empty\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
